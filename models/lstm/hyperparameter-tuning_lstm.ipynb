{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3yP00vwb6MO"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "sys.path.append('/mnt/home/rheinrich/taaowpf')\n",
    "\n",
    "from data.lstm.wpf_dataset_single_turbine_gefcom import WPF_SingleTurbine_DataModule\n",
    "from models.lstm.lstm import WPF_AutoencoderLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONPATH\"] = '/mnt/home/rheinrich/taaowpf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "import logging\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(\n",
    "    object_store_memory=32000000000,\n",
    "    include_dashboard=False,\n",
    "    ignore_reinit_error=True,\n",
    "    num_cpus=32,\n",
    "    num_gpus=3,\n",
    "    _temp_dir=\"/mnt/home/rheinrich/ray/tmp\",\n",
    "    logging_level =logging.WARNING,\n",
    "    log_to_driver = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/releases-1.11.0/tune/tutorials/tune-pytorch-lightning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASHA Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used for Training during Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_tune(config, data_dir, checkpoint_dir=None, num_gpus=0):\n",
    "    \n",
    "    # initiate DataModule\n",
    "    datamodule = WPF_SingleTurbine_DataModule(data_dir = data_dir,\n",
    "                                              forecast_horizon = config['forecast_horizon'],  \n",
    "                                              n_past_timesteps = config['n_past_timesteps'],\n",
    "                                              batch_size = config['batch_size'], \n",
    "                                              num_workers = config['num_workers_datamodule'],\n",
    "                                             )\n",
    "\n",
    "    # define callback for Early Stopping\n",
    "    early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                min_delta = 1e-6,\n",
    "                                                patience = 15)\n",
    "    \n",
    "    # use the Ray Tune callback TuneReportCallback to report metrics back to Tune after each validation epoch\n",
    "    tune_report_callback = TuneReportCallback({\"loss\": \"val_loss\", \"val_rmse\": \"val_rmse\"}, on=\"validation_end\")\n",
    "    \n",
    "    # initiate model\n",
    "    model = WPF_AutoencoderLSTM(forecast_horizon = config['forecast_horizon'],\n",
    "                                n_past_timesteps = config['n_past_timesteps'],\n",
    "                                hidden_size = config['hidden_size'],\n",
    "                                num_layers = config['num_layers'],\n",
    "                                learning_rate= config['learning_rate'],\n",
    "                                p_adv_training = config['p_adv_training'],\n",
    "                                eps_adv_training = config['eps_adv_training'],\n",
    "                                step_num_adv_training = config['step_num_adv_training'],\n",
    "                                norm_adv_training = config['norm_adv_training'])\n",
    "    \n",
    "    # initiate Trainer\n",
    "    trainer = pl.Trainer(max_epochs = config['max_epochs'],\n",
    "                         devices = 1,\n",
    "                         accelerator = 'gpu',\n",
    "                         logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "                         enable_progress_bar = False,\n",
    "                         enable_checkpointing=False, # otherwise memory gets too large during hyperparameter tuning\n",
    "                         callbacks=[tune_report_callback, early_stopping],\n",
    "                        )\n",
    "    \n",
    "    # fit model\n",
    "    trainer.fit(model, datamodule = datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparams_asha(data_dir, num_samples, gpus_per_trial=0, cpus_per_trial=1, grace_period_asha = 10):\n",
    "    # configure the search space\n",
    "    config = {\n",
    "        \"n_past_timesteps\": tune.randint(lower = 1, upper = 25),\n",
    "        \"hidden_size\": tune.choice([32, 64, 96, 128, 160, 192, 224, 256]),\n",
    "        \"num_layers\": tune.randint(lower = 1, upper = 4),\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"forecast_horizon\": 8,\n",
    "        \"max_epochs\": 100,\n",
    "        \"batch_size\": 256,\n",
    "        \"num_workers_datamodule\": 0,\n",
    "        \"p_adv_training\": 0.0, # probability is zero, so no adversarial training is used for hyperparameter tuning\n",
    "        \"eps_adv_training\": 0.1,\n",
    "        \"step_num_adv_training\": 100,\n",
    "        \"norm_adv_training\": 'Linf',\n",
    "    }\n",
    "\n",
    "    # select a scheduler / algorithm for hyperparameter tuning\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=config['max_epochs'],\n",
    "        grace_period=grace_period_asha\n",
    "    )\n",
    "\n",
    "    # define the desired CLI output\n",
    "    reporter = CLIReporter(\n",
    "        #parameter_columns= list(config.keys()), # shows all hyperparemters\n",
    "        parameter_columns= [\"batch_size\"], \n",
    "        metric_columns=[\"loss\", \"val_rmse\", \"training_iteration\"])\n",
    "\n",
    "    # pass constants to the train function\n",
    "    train_fn_with_parameters = tune.with_parameters(train_model_tune,\n",
    "                                                    num_gpus=gpus_per_trial,\n",
    "                                                    data_dir = data_dir)\n",
    "    \n",
    "    # specify how many resources Tune should request for each trial\n",
    "    resources_per_trial = {\"cpu\": cpus_per_trial, \"gpu\": gpus_per_trial}\n",
    "\n",
    "    # start Tune\n",
    "    analysis = tune.run(train_fn_with_parameters,\n",
    "                        resources_per_trial=resources_per_trial,\n",
    "                        metric=\"loss\",\n",
    "                        mode=\"min\",\n",
    "                        config=config,\n",
    "                        num_samples=num_samples,\n",
    "                        scheduler=scheduler,\n",
    "                        progress_reporter=reporter,\n",
    "                        name=\"tune_single-turbine_model_gefcom_202212\", \n",
    "                        local_dir = \"/mnt/home/rheinrich/ray/ray_results/taaowpf\",\n",
    "                        keep_checkpoints_num = 100,\n",
    "                        checkpoint_score_attr = 'min-loss',\n",
    "                        verbose = 1\n",
    "                       )\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/mnt/home/rheinrich/taaowpf/data/lstm/Gefcom2014_Wind/gefcom2014_W_100m_zone1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_hyperparams_asha(data_dir = data_dir, \n",
    "                      num_samples=1000, # number of times to sample from the hyperparameter space\n",
    "                      gpus_per_trial=1,\n",
    "                      cpus_per_trial = 1,\n",
    "                      grace_period_asha = 20\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Results of Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to Experiment (make sure, every hyperparameter tuning experiment is stored in a separate folder!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = \"/mnt/home/rheinrich/ray/ray_results/taaowpf/tune_single-turbine_model_gefcom_202212\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import ExperimentAnalysis\n",
    "analysis = ExperimentAnalysis(experiment_path, default_metric = \"loss\", default_mode = \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Simple_cnn_Binary_BirdCLEF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
